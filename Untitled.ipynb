{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "import argparse\n",
    "import glob\n",
    "import numpy as np\n",
    "import yaml\n",
    "import tqdm\n",
    "\n",
    "from addict import Dict\n",
    "\n",
    "from dataset import PartAffordanceDataset, ToTensor, CenterCrop, Normalize\n",
    "from dataset import Resize, RandomFlip, RandomRotate, RandomCrop, reverse_normalize\n",
    "from model.drn import drn_c_58\n",
    "from model.drn_max import drn_c_58_max\n",
    "from utils.cam import CAM, GradCAM\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "\"\"\" \n",
    "for the details of SegNet, please refer to this paper:\n",
    "\n",
    "Badrinarayanan, V., Kendall, A., & Cipolla, R. (2017). \n",
    "SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation.\n",
    "IEEE Transactions on Pattern Analysis and Machine Intelligence, 39(12), 2481â€“2495. \n",
    "https://doi.org/10.1109/TPAMI.2016.2644615\n",
    "\n",
    "\n",
    "SegNet Basic is a smaller version of SegNet\n",
    "Please refer to this repository:\n",
    "https://github.com/0bserver07/Keras-SegNet-Basic\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channel, out_channel, kernel_size=3, padding=1)\n",
    "        self.bn = nn.BatchNorm2d(out_channel)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = F.relu(x)\n",
    "        x, idx = F.max_pool2d(x, kernel_size=2, stride=2, return_indices=True)\n",
    "        return x, idx\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channel, out_channel, kernel_size=3, padding=1)\n",
    "        self.bn = nn.BatchNorm2d(out_channel)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = F.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SegNetBasic(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder1 = Encoder(in_channel, 64)\n",
    "        self.encoder2 = Encoder(64, 80)\n",
    "        self.encoder3 = Encoder(80, 96)\n",
    "        self.encoder4 = Encoder(96, 128)\n",
    "        \n",
    "        self.decoder1 = Decoder(128, 96)\n",
    "        self.decoder2 = Decoder(96, 80)\n",
    "        self.decoder3 = Decoder(80, 64)\n",
    "        self.decoder4 = Decoder(64, out_channel)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        size1 = x.size()\n",
    "        x, idx1 = self.encoder1(x)\n",
    "\n",
    "        size2 = x.size()\n",
    "        x, idx2 = self.encoder2(x)\n",
    "\n",
    "        size3 = x.size()\n",
    "        x, idx3 = self.encoder3(x)\n",
    "        \n",
    "        size4 = x.size()\n",
    "        x, idx4 = self.encoder4(x)\n",
    "\n",
    "        x = F.max_unpool2d(x, idx4, kernel_size=2, stride=2, output_size=size4)\n",
    "        x = self.decoder1(x)\n",
    "        \n",
    "        x = F.max_unpool2d(x, idx3, kernel_size=2, stride=2, output_size=size3)\n",
    "        x = self.decoder2(x)\n",
    "\n",
    "        x = F.max_unpool2d(x, idx2, kernel_size=2, stride=2, output_size=size2)\n",
    "        x = self.decoder3(x)\n",
    "\n",
    "        x = F.max_unpool2d(x, idx1, kernel_size=2, stride=2, output_size=size1)\n",
    "        x = self.decoder4(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SegNetBasic(3, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load('final_model.prm', map_location=lambda storage, loc: storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" normalize images \"\"\"\n",
    "class Normalize(object):\n",
    "    def __init__(self, mean=[0.2191, 0.2349, 0.3598], std=[0.1243, 0.1171, 0.0748]):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image = sample['image']\n",
    "        image = transforms.functional.normalize(image, self.mean, self.std)\n",
    "        sample['image'] = image\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = Dict(yaml.safe_load(open('./result/drn_c_58_max/config.yaml')))\n",
    "\n",
    "\"\"\" DataLoader \"\"\"\n",
    "train_transform = transforms.Compose([\n",
    "    ToTensor(CONFIG),\n",
    "    Normalize()\n",
    "])\n",
    "\n",
    "train_data = PartAffordanceDataset(\n",
    "    CONFIG.train_data, config=CONFIG, transform=train_transform, mode='test', make_cam_label=True)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_data, batch_size=5, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = torch.tensor([[0, 0, 0],         # class 0 'background'  black\n",
    "                        [255, 0, 0],       # class 1 'grasp'       red\n",
    "                        [255, 255, 0],     # class 2 'cut'         yellow\n",
    "                        [0, 255, 0],       # class 3 'scoop'       green\n",
    "                        [0, 255, 255],     # class 4 'contain'     sky blue\n",
    "                        [0, 0, 255],       # class 5 'pound'       blue\n",
    "                        [255, 0, 255],     # class 6 'support'     purple\n",
    "                        [255, 255, 255]    # class 7 'wrap grasp'  white\n",
    "                        ])\n",
    "\n",
    "# convert class prediction to the mask\n",
    "def class_to_mask(cls):\n",
    "    \n",
    "    mask = colors[cls].transpose(1, 2).transpose(1, 3)\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pydensecrf.densecrf as dcrf\n",
    "import pydensecrf.utils as utils\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "class DenseCRF(object):\n",
    "    def __init__(self, iter_max=10, pos_w=3, pos_xy_std=1, bi_w=4, bi_xy_std=67, bi_rgb_std=3):\n",
    "        self.iter_max = iter_max\n",
    "        self.pos_w = pos_w\n",
    "        self.pos_xy_std = pos_xy_std\n",
    "        self.bi_w = bi_w\n",
    "        self.bi_xy_std = bi_xy_std\n",
    "        self.bi_rgb_std = bi_rgb_std\n",
    "\n",
    "    def __call__(self, image, probmap):\n",
    "        C, H, W = probmap.shape\n",
    "\n",
    "        U = utils.unary_from_softmax(probmap)\n",
    "        U = np.ascontiguousarray(U)\n",
    "\n",
    "        image = np.ascontiguousarray(image)\n",
    "\n",
    "        d = dcrf.DenseCRF2D(W, H, C)\n",
    "        d.setUnaryEnergy(U)\n",
    "        d.addPairwiseGaussian(sxy=self.pos_xy_std, compat=self.pos_w)\n",
    "        d.addPairwiseBilateral(\n",
    "            sxy=self.bi_xy_std, srgb=self.bi_rgb_std, rgbim=image, compat=self.bi_w\n",
    "        )\n",
    "\n",
    "        Q = d.inference(self.iter_max)\n",
    "        Q = np.array(Q).reshape((C, H, W))\n",
    "\n",
    "        return Q\n",
    "\n",
    "crf = DenseCRF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 8, 480, 640])\n"
     ]
    }
   ],
   "source": [
    "for sample in train_loader:\n",
    "    img, y = sample['image'], sample['label']\n",
    "    prob = model(img)\n",
    "    _, h = prob.max(1)\n",
    "    \n",
    "    true_mask = class_to_mask(y).to('cpu')\n",
    "    pred_mask = class_to_mask(h).to('cpu')\n",
    "    \n",
    "    save_image(true_mask, 'true_mask.jpg')\n",
    "    save_image(pred_mask, 'pred_mask.jpg')\n",
    "    \n",
    "    img = (img * 255).to('cpu').numpy().astype(np.uint8).transpose(0, 2, 3, 1)\n",
    "    prob = torch.softmax(prob, 1)    # shape => (N, C, h, w)\n",
    "    probmap = prob.detach().numpy()\n",
    "\n",
    "    # CRF\n",
    "    Q = Parallel(n_jobs=-2)([\n",
    "        delayed(crf)(*pair) for pair in zip(img, probmap)\n",
    "    ])\n",
    "    Q = torch.tensor(Q).to('cpu')    # shape => (N, C, h, w)\n",
    "    _, Q = Q.max(1)\n",
    "    \n",
    "    Q = class_to_mask(Q)\n",
    "    save_image(Q, 'crf.jpg')\n",
    "    \n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "import argparse\n",
    "import glob\n",
    "import numpy as np\n",
    "import yaml\n",
    "import tqdm\n",
    "\n",
    "from addict import Dict\n",
    "from PIL import Image\n",
    "\n",
    "from dataset import PartAffordanceDataset, ToTensor, CenterCrop, Normalize\n",
    "from dataset import Resize, RandomFlip, RandomRotate, RandomCrop, reverse_normalize\n",
    "from model.drn import drn_c_58\n",
    "from model.drn_max import drn_c_58_max, drn_d_105_max\n",
    "from utils.cam import CAM, GradCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = Dict(\n",
    "    yaml.safe_load(open('./result/drn_c_58_max/config.yaml')))\n",
    "\n",
    "\"\"\" DataLoader \"\"\"\n",
    "train_transform = transforms.Compose([\n",
    "    ToTensor(CONFIG),\n",
    "    Normalize()\n",
    "])\n",
    "\n",
    "train_data = PartAffordanceDataset(\n",
    "    CONFIG.train_data, config=CONFIG, transform=train_transform, mode='test', make_cam_label=True)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_data, batch_size=1, shuffle=True, num_workers=2)\n",
    "\n",
    "model = drn_c_58_max(\n",
    "        pretrained=False, num_obj=CONFIG.obj_classes, num_aff=CONFIG.aff_classes)\n",
    "\n",
    "state_dict = torch.load(CONFIG.result_path + '/best_accuracy_model.prm',\n",
    "                        map_location=lambda storage, loc: storage)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "\n",
    "target_layer_obj = model.obj_conv\n",
    "target_layer_aff = model.aff_conv\n",
    "\n",
    "# choose CAM or GradCAM\n",
    "wrapped_model = CAM(model, target_layer_obj, target_layer_aff)\n",
    "# wrapped_model = GradCAM(model, target_layer_obj, target_layer_aff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, obj_label, aff_label = sample['image'], sample['obj_label'], sample['aff_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted object ids tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<IndexPutBackward>)\n",
      "predicted affordance ids tensor([[1., 1., 1., 0., 0., 0., 0., 0.]], grad_fn=<IndexPutBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuchi/.pyenv/versions/3.6.5/envs/torch/lib/python3.6/site-packages/torch/nn/functional.py:2423: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    }
   ],
   "source": [
    "_, _, H, W = x.shape\n",
    "pred_obj, pred_aff = wrapped_model(x)\n",
    "\n",
    "weight_fc_obj = list(\n",
    "    wrapped_model.model._modules.get('obj_fc').parameters())[0].to('cpu').data\n",
    "weight_fc_aff = list(\n",
    "    wrapped_model.model._modules.get('aff_fc').parameters())[0].to('cpu').data\n",
    "\n",
    "cam_obj = F.conv2d(\n",
    "    wrapped_model.values_obj.activations, weight=weight_fc_obj[:, :, None, None])\n",
    "cam_aff = F.conv2d(\n",
    "    wrapped_model.values_aff.activations, weight=weight_fc_aff[:, :, None, None])\n",
    "\n",
    "# resize\n",
    "cam_obj = F.interpolate(\n",
    "    cam_obj, (H, W), mode='bilinear').view(-1, H, W)\n",
    "cam_aff = F.interpolate(\n",
    "    cam_aff, (H, W), mode='bilinear').view(-1, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_aff = cam_aff[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 480, 640])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cam_aff.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_obj = cam_obj[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([17, 480, 640])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cam_obj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_aff = cam_aff.data.numpy()\n",
    "cam_obj = cam_obj.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_label, aff_label = obj_label[0, 1:].numpy(), aff_label[0, 1:].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "arrays used as indices must be of integer (or boolean) type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-fe27c478a859>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcam_obj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobj_label\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: arrays used as indices must be of integer (or boolean) type"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
