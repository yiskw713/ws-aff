{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "import argparse\n",
    "import glob\n",
    "import numpy as np\n",
    "import yaml\n",
    "import tqdm\n",
    "\n",
    "from addict import Dict\n",
    "\n",
    "from dataset import PartAffordanceDataset, ToTensor, CenterCrop, Normalize\n",
    "from dataset import Resize, RandomFlip, RandomRotate, RandomCrop, reverse_normalize\n",
    "from model.drn import drn_c_58\n",
    "from model.drn_max import drn_c_58_max\n",
    "from utils.cam import CAM, GradCAM\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "\"\"\" \n",
    "for the details of SegNet, please refer to this paper:\n",
    "\n",
    "Badrinarayanan, V., Kendall, A., & Cipolla, R. (2017). \n",
    "SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation.\n",
    "IEEE Transactions on Pattern Analysis and Machine Intelligence, 39(12), 2481â€“2495. \n",
    "https://doi.org/10.1109/TPAMI.2016.2644615\n",
    "\n",
    "\n",
    "SegNet Basic is a smaller version of SegNet\n",
    "Please refer to this repository:\n",
    "https://github.com/0bserver07/Keras-SegNet-Basic\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channel, out_channel, kernel_size=3, padding=1)\n",
    "        self.bn = nn.BatchNorm2d(out_channel)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = F.relu(x)\n",
    "        x, idx = F.max_pool2d(x, kernel_size=2, stride=2, return_indices=True)\n",
    "        return x, idx\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channel, out_channel, kernel_size=3, padding=1)\n",
    "        self.bn = nn.BatchNorm2d(out_channel)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = F.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SegNetBasic(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder1 = Encoder(in_channel, 64)\n",
    "        self.encoder2 = Encoder(64, 80)\n",
    "        self.encoder3 = Encoder(80, 96)\n",
    "        self.encoder4 = Encoder(96, 128)\n",
    "        \n",
    "        self.decoder1 = Decoder(128, 96)\n",
    "        self.decoder2 = Decoder(96, 80)\n",
    "        self.decoder3 = Decoder(80, 64)\n",
    "        self.decoder4 = Decoder(64, out_channel)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        size1 = x.size()\n",
    "        x, idx1 = self.encoder1(x)\n",
    "\n",
    "        size2 = x.size()\n",
    "        x, idx2 = self.encoder2(x)\n",
    "\n",
    "        size3 = x.size()\n",
    "        x, idx3 = self.encoder3(x)\n",
    "        \n",
    "        size4 = x.size()\n",
    "        x, idx4 = self.encoder4(x)\n",
    "\n",
    "        x = F.max_unpool2d(x, idx4, kernel_size=2, stride=2, output_size=size4)\n",
    "        x = self.decoder1(x)\n",
    "        \n",
    "        x = F.max_unpool2d(x, idx3, kernel_size=2, stride=2, output_size=size3)\n",
    "        x = self.decoder2(x)\n",
    "\n",
    "        x = F.max_unpool2d(x, idx2, kernel_size=2, stride=2, output_size=size2)\n",
    "        x = self.decoder3(x)\n",
    "\n",
    "        x = F.max_unpool2d(x, idx1, kernel_size=2, stride=2, output_size=size1)\n",
    "        x = self.decoder4(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SegNetBasic(3, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load('final_model.prm', map_location=lambda storage, loc: storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" normalize images \"\"\"\n",
    "class Normalize(object):\n",
    "    def __init__(self, mean=[0.2191, 0.2349, 0.3598], std=[0.1243, 0.1171, 0.0748]):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image = sample['image']\n",
    "        image = transforms.functional.normalize(image, self.mean, self.std)\n",
    "        sample['image'] = image\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = Dict(yaml.safe_load(open('./result/drn_c_58_max/config.yaml')))\n",
    "\n",
    "\"\"\" DataLoader \"\"\"\n",
    "train_transform = transforms.Compose([\n",
    "    ToTensor(CONFIG),\n",
    "    Normalize()\n",
    "])\n",
    "\n",
    "train_data = PartAffordanceDataset(\n",
    "    CONFIG.train_data, config=CONFIG, transform=train_transform, mode='test', make_cam_label=True)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_data, batch_size=5, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = torch.tensor([[0, 0, 0],         # class 0 'background'  black\n",
    "                        [255, 0, 0],       # class 1 'grasp'       red\n",
    "                        [255, 255, 0],     # class 2 'cut'         yellow\n",
    "                        [0, 255, 0],       # class 3 'scoop'       green\n",
    "                        [0, 255, 255],     # class 4 'contain'     sky blue\n",
    "                        [0, 0, 255],       # class 5 'pound'       blue\n",
    "                        [255, 0, 255],     # class 6 'support'     purple\n",
    "                        [255, 255, 255]    # class 7 'wrap grasp'  white\n",
    "                        ])\n",
    "\n",
    "# convert class prediction to the mask\n",
    "def class_to_mask(cls):\n",
    "    \n",
    "    mask = colors[cls].transpose(1, 2).transpose(1, 3)\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pydensecrf.densecrf as dcrf\n",
    "import pydensecrf.utils as utils\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "class DenseCRF(object):\n",
    "    def __init__(self, iter_max=10, pos_w=3, pos_xy_std=1, bi_w=4, bi_xy_std=67, bi_rgb_std=3):\n",
    "        self.iter_max = iter_max\n",
    "        self.pos_w = pos_w\n",
    "        self.pos_xy_std = pos_xy_std\n",
    "        self.bi_w = bi_w\n",
    "        self.bi_xy_std = bi_xy_std\n",
    "        self.bi_rgb_std = bi_rgb_std\n",
    "\n",
    "    def __call__(self, image, probmap):\n",
    "        C, H, W = probmap.shape\n",
    "\n",
    "        U = utils.unary_from_softmax(probmap)\n",
    "        U = np.ascontiguousarray(U)\n",
    "\n",
    "        image = np.ascontiguousarray(image)\n",
    "\n",
    "        d = dcrf.DenseCRF2D(W, H, C)\n",
    "        d.setUnaryEnergy(U)\n",
    "        d.addPairwiseGaussian(sxy=self.pos_xy_std, compat=self.pos_w)\n",
    "        d.addPairwiseBilateral(\n",
    "            sxy=self.bi_xy_std, srgb=self.bi_rgb_std, rgbim=image, compat=self.bi_w\n",
    "        )\n",
    "\n",
    "        Q = d.inference(self.iter_max)\n",
    "        Q = np.array(Q).reshape((C, H, W))\n",
    "\n",
    "        return Q\n",
    "\n",
    "crf = DenseCRF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 8, 480, 640])\n"
     ]
    }
   ],
   "source": [
    "for sample in train_loader:\n",
    "    img, y = sample['image'], sample['label']\n",
    "    prob = model(img)\n",
    "    _, h = prob.max(1)\n",
    "    \n",
    "    true_mask = class_to_mask(y).to('cpu')\n",
    "    pred_mask = class_to_mask(h).to('cpu')\n",
    "    \n",
    "    save_image(true_mask, 'true_mask.jpg')\n",
    "    save_image(pred_mask, 'pred_mask.jpg')\n",
    "    \n",
    "    img = (img * 255).to('cpu').numpy().astype(np.uint8).transpose(0, 2, 3, 1)\n",
    "    prob = torch.softmax(prob, 1)    # shape => (N, C, h, w)\n",
    "    probmap = prob.detach().numpy()\n",
    "\n",
    "    # CRF\n",
    "    Q = Parallel(n_jobs=-2)([\n",
    "        delayed(crf)(*pair) for pair in zip(img, probmap)\n",
    "    ])\n",
    "    Q = torch.tensor(Q).to('cpu')    # shape => (N, C, h, w)\n",
    "    _, Q = Q.max(1)\n",
    "    \n",
    "    Q = class_to_mask(Q)\n",
    "    save_image(Q, 'crf.jpg')\n",
    "    \n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# affordance class weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = glob.glob(\"./part-affordance-dataset/tools/*/*aff_cam_label.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_dict = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0, 255:0}\n",
    "\n",
    "for p in path:\n",
    "    img = Image.open(p)\n",
    "    img = np.asarray(img)\n",
    "    num, cnt = np.unique(img, return_counts=True)\n",
    "    \n",
    "    for n, c in zip(num, cnt):\n",
    "        cnt_dict[n] += c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 7321360010,\n",
       " 1: 18311032,\n",
       " 2: 4406809,\n",
       " 3: 1983549,\n",
       " 4: 4377947,\n",
       " 5: 1132548,\n",
       " 6: 1074818,\n",
       " 7: 6837994,\n",
       " 255: 1501084893}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.9797e-04, 2.3909e-01, 9.9345e-01, 2.2071e+00, 1.0000e+00, 3.8656e+00,\n",
      "        4.0732e+00, 6.4024e-01])\n"
     ]
    }
   ],
   "source": [
    "class_num = torch.tensor([7321360010, 18311032, 4406809, 1983549,\n",
    "                        4377947, 1132548, 1074818, 6837994])\n",
    "total = class_num.sum().item()\n",
    "frequency = class_num.float() / total\n",
    "median = torch.median(frequency)\n",
    "class_weight = median / frequency\n",
    "print(class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "class_num = torch.tensor([7321360010, 18311032, 4406809, 1983549,\n",
    "                        4377947, 1132548, 1074818, 6837994])\n",
    "class_weight = 1. / class_num\n",
    "print(class_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# object class weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = glob.glob(\"./part-affordance-dataset/tools/*/*obj_cam_label.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_dict = {}\n",
    "for i in range(18):\n",
    "    cnt_dict[i]=0\n",
    "cnt_dict[255] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in path:\n",
    "    img = Image.open(p)\n",
    "    img = np.asarray(img)\n",
    "    num, cnt = np.unique(img, return_counts=True)\n",
    "    \n",
    "    for n, c in zip(num, cnt):\n",
    "        cnt_dict[n] += c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 7727538416,\n",
       " 1: 463493,\n",
       " 2: 341973,\n",
       " 3: 396001,\n",
       " 4: 1550892,\n",
       " 5: 706330,\n",
       " 6: 499596,\n",
       " 7: 6977046,\n",
       " 8: 180450,\n",
       " 9: 315860,\n",
       " 10: 708978,\n",
       " 11: 414156,\n",
       " 12: 312187,\n",
       " 13: 227372,\n",
       " 14: 901299,\n",
       " 15: 289115,\n",
       " 16: 573954,\n",
       " 17: 673327,\n",
       " 255: 1117499155}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_num = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7727538416, 15532029]\n"
     ]
    }
   ],
   "source": [
    "class_num.append(cnt_dict[0])\n",
    "c = 0\n",
    "for i in range(1,18):\n",
    "    c += cnt_dict[i]\n",
    "class_num.append(c)\n",
    "print(class_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0020, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "class_num = torch.tensor(class_num)\n",
    "total = class_num.sum().item()\n",
    "frequency = class_num.float() / total\n",
    "median = torch.median(frequency)\n",
    "class_weight = median / frequency\n",
    "print(class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
