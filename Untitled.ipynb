{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "import argparse\n",
    "import glob\n",
    "import numpy as np\n",
    "import yaml\n",
    "import tqdm\n",
    "\n",
    "from addict import Dict\n",
    "\n",
    "from dataset import PartAffordanceDataset, ToTensor, CenterCrop, Normalize\n",
    "from dataset import Resize, RandomFlip, RandomRotate, RandomCrop, reverse_normalize\n",
    "from model.drn import drn_c_58\n",
    "from model.drn_max import drn_c_58_max\n",
    "from utils.cam import CAM, GradCAM\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "\"\"\" \n",
    "for the details of SegNet, please refer to this paper:\n",
    "\n",
    "Badrinarayanan, V., Kendall, A., & Cipolla, R. (2017). \n",
    "SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation.\n",
    "IEEE Transactions on Pattern Analysis and Machine Intelligence, 39(12), 2481â€“2495. \n",
    "https://doi.org/10.1109/TPAMI.2016.2644615\n",
    "\n",
    "\n",
    "SegNet Basic is a smaller version of SegNet\n",
    "Please refer to this repository:\n",
    "https://github.com/0bserver07/Keras-SegNet-Basic\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channel, out_channel, kernel_size=3, padding=1)\n",
    "        self.bn = nn.BatchNorm2d(out_channel)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = F.relu(x)\n",
    "        x, idx = F.max_pool2d(x, kernel_size=2, stride=2, return_indices=True)\n",
    "        return x, idx\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channel, out_channel, kernel_size=3, padding=1)\n",
    "        self.bn = nn.BatchNorm2d(out_channel)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = F.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SegNetBasic(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder1 = Encoder(in_channel, 64)\n",
    "        self.encoder2 = Encoder(64, 80)\n",
    "        self.encoder3 = Encoder(80, 96)\n",
    "        self.encoder4 = Encoder(96, 128)\n",
    "        \n",
    "        self.decoder1 = Decoder(128, 96)\n",
    "        self.decoder2 = Decoder(96, 80)\n",
    "        self.decoder3 = Decoder(80, 64)\n",
    "        self.decoder4 = Decoder(64, out_channel)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        size1 = x.size()\n",
    "        x, idx1 = self.encoder1(x)\n",
    "\n",
    "        size2 = x.size()\n",
    "        x, idx2 = self.encoder2(x)\n",
    "\n",
    "        size3 = x.size()\n",
    "        x, idx3 = self.encoder3(x)\n",
    "        \n",
    "        size4 = x.size()\n",
    "        x, idx4 = self.encoder4(x)\n",
    "\n",
    "        x = F.max_unpool2d(x, idx4, kernel_size=2, stride=2, output_size=size4)\n",
    "        x = self.decoder1(x)\n",
    "        \n",
    "        x = F.max_unpool2d(x, idx3, kernel_size=2, stride=2, output_size=size3)\n",
    "        x = self.decoder2(x)\n",
    "\n",
    "        x = F.max_unpool2d(x, idx2, kernel_size=2, stride=2, output_size=size2)\n",
    "        x = self.decoder3(x)\n",
    "\n",
    "        x = F.max_unpool2d(x, idx1, kernel_size=2, stride=2, output_size=size1)\n",
    "        x = self.decoder4(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SegNetBasic(3, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load('final_model.prm', map_location=lambda storage, loc: storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" normalize images \"\"\"\n",
    "class Normalize(object):\n",
    "    def __init__(self, mean=[0.2191, 0.2349, 0.3598], std=[0.1243, 0.1171, 0.0748]):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image = sample['image']\n",
    "        image = transforms.functional.normalize(image, self.mean, self.std)\n",
    "        sample['image'] = image\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = Dict(yaml.safe_load(open('./result/drn_c_58_max/config.yaml')))\n",
    "\n",
    "\"\"\" DataLoader \"\"\"\n",
    "train_transform = transforms.Compose([\n",
    "    ToTensor(CONFIG),\n",
    "    Normalize()\n",
    "])\n",
    "\n",
    "train_data = PartAffordanceDataset(\n",
    "    CONFIG.train_data, config=CONFIG, transform=train_transform, mode='test', make_cam_label=True)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_data, batch_size=5, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = torch.tensor([[0, 0, 0],         # class 0 'background'  black\n",
    "                        [255, 0, 0],       # class 1 'grasp'       red\n",
    "                        [255, 255, 0],     # class 2 'cut'         yellow\n",
    "                        [0, 255, 0],       # class 3 'scoop'       green\n",
    "                        [0, 255, 255],     # class 4 'contain'     sky blue\n",
    "                        [0, 0, 255],       # class 5 'pound'       blue\n",
    "                        [255, 0, 255],     # class 6 'support'     purple\n",
    "                        [255, 255, 255]    # class 7 'wrap grasp'  white\n",
    "                        ])\n",
    "\n",
    "# convert class prediction to the mask\n",
    "def class_to_mask(cls):\n",
    "    \n",
    "    mask = colors[cls].transpose(1, 2).transpose(1, 3)\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pydensecrf.densecrf as dcrf\n",
    "import pydensecrf.utils as utils\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "class DenseCRF(object):\n",
    "    def __init__(self, iter_max=10, pos_w=3, pos_xy_std=1, bi_w=4, bi_xy_std=67, bi_rgb_std=3):\n",
    "        self.iter_max = iter_max\n",
    "        self.pos_w = pos_w\n",
    "        self.pos_xy_std = pos_xy_std\n",
    "        self.bi_w = bi_w\n",
    "        self.bi_xy_std = bi_xy_std\n",
    "        self.bi_rgb_std = bi_rgb_std\n",
    "\n",
    "    def __call__(self, image, probmap):\n",
    "        C, H, W = probmap.shape\n",
    "\n",
    "        U = utils.unary_from_softmax(probmap)\n",
    "        U = np.ascontiguousarray(U)\n",
    "\n",
    "        image = np.ascontiguousarray(image)\n",
    "\n",
    "        d = dcrf.DenseCRF2D(W, H, C)\n",
    "        d.setUnaryEnergy(U)\n",
    "        d.addPairwiseGaussian(sxy=self.pos_xy_std, compat=self.pos_w)\n",
    "        d.addPairwiseBilateral(\n",
    "            sxy=self.bi_xy_std, srgb=self.bi_rgb_std, rgbim=image, compat=self.bi_w\n",
    "        )\n",
    "\n",
    "        Q = d.inference(self.iter_max)\n",
    "        Q = np.array(Q).reshape((C, H, W))\n",
    "\n",
    "        return Q\n",
    "\n",
    "crf = DenseCRF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 8, 480, 640])\n"
     ]
    }
   ],
   "source": [
    "for sample in train_loader:\n",
    "    img, y = sample['image'], sample['label']\n",
    "    prob = model(img)\n",
    "    _, h = prob.max(1)\n",
    "    \n",
    "    true_mask = class_to_mask(y).to('cpu')\n",
    "    pred_mask = class_to_mask(h).to('cpu')\n",
    "    \n",
    "    save_image(true_mask, 'true_mask.jpg')\n",
    "    save_image(pred_mask, 'pred_mask.jpg')\n",
    "    \n",
    "    img = (img * 255).to('cpu').numpy().astype(np.uint8).transpose(0, 2, 3, 1)\n",
    "    prob = torch.softmax(prob, 1)    # shape => (N, C, h, w)\n",
    "    probmap = prob.detach().numpy()\n",
    "\n",
    "    # CRF\n",
    "    Q = Parallel(n_jobs=-2)([\n",
    "        delayed(crf)(*pair) for pair in zip(img, probmap)\n",
    "    ])\n",
    "    Q = torch.tensor(Q).to('cpu')    # shape => (N, C, h, w)\n",
    "    _, Q = Q.max(1)\n",
    "    \n",
    "    Q = class_to_mask(Q)\n",
    "    save_image(Q, 'crf.jpg')\n",
    "    \n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "import argparse\n",
    "import glob\n",
    "import numpy as np\n",
    "import yaml\n",
    "import tqdm\n",
    "\n",
    "from addict import Dict\n",
    "from PIL import Image\n",
    "\n",
    "from dataset import PartAffordanceDataset, ToTensor, CenterCrop, Normalize\n",
    "from dataset import Resize, RandomFlip, RandomRotate, RandomCrop, reverse_normalize\n",
    "from model.drn import drn_c_58\n",
    "from model.drn_max import drn_c_58_max, drn_d_105_max\n",
    "from utils.cam import CAM, GradCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = Dict(\n",
    "    yaml.safe_load(open('./result/drn_c_58_max/config.yaml')))\n",
    "\n",
    "\"\"\" DataLoader \"\"\"\n",
    "train_transform = transforms.Compose([\n",
    "    ToTensor(CONFIG),\n",
    "    Normalize()\n",
    "])\n",
    "\n",
    "train_data = PartAffordanceDataset(\n",
    "    CONFIG.train_data, config=CONFIG, transform=train_transform, mode='test', make_cam_label=True)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_data, batch_size=1, shuffle=True, num_workers=2)\n",
    "\n",
    "model = drn_c_58_max(\n",
    "        pretrained=False, num_obj=CONFIG.obj_classes, num_aff=CONFIG.aff_classes)\n",
    "\n",
    "state_dict = torch.load(CONFIG.result_path + '/best_accuracy_model.prm',\n",
    "                        map_location=lambda storage, loc: storage)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "\n",
    "target_layer_obj = model.obj_conv\n",
    "target_layer_aff = model.aff_conv\n",
    "\n",
    "# choose CAM or GradCAM\n",
    "wrapped_model = CAM(model, target_layer_obj, target_layer_aff)\n",
    "# wrapped_model = GradCAM(model, target_layer_obj, target_layer_aff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, obj_label, aff_label = sample['image'], sample['obj_label'], sample['aff_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted object ids tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<IndexPutBackward>)\n",
      "predicted affordance ids tensor([[1., 1., 1., 0., 0., 0., 0., 0.]], grad_fn=<IndexPutBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuchi/.pyenv/versions/3.6.5/envs/torch/lib/python3.6/site-packages/torch/nn/functional.py:2423: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    }
   ],
   "source": [
    "_, _, H, W = x.shape\n",
    "pred_obj, pred_aff = wrapped_model(x)\n",
    "\n",
    "weight_fc_obj = list(\n",
    "    wrapped_model.model._modules.get('obj_fc').parameters())[0].to('cpu').data\n",
    "weight_fc_aff = list(\n",
    "    wrapped_model.model._modules.get('aff_fc').parameters())[0].to('cpu').data\n",
    "\n",
    "cam_obj = F.conv2d(\n",
    "    wrapped_model.values_obj.activations, weight=weight_fc_obj[:, :, None, None])\n",
    "cam_aff = F.conv2d(\n",
    "    wrapped_model.values_aff.activations, weight=weight_fc_aff[:, :, None, None])\n",
    "\n",
    "# resize\n",
    "cam_obj = F.interpolate(\n",
    "    cam_obj, (H, W), mode='bilinear').view(-1, H, W)\n",
    "cam_aff = F.interpolate(\n",
    "    cam_aff, (H, W), mode='bilinear').view(-1, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_aff = cam_aff[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 480, 640])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cam_aff.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_obj = cam_obj[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([17, 480, 640])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cam_obj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_aff = cam_aff.data.numpy()\n",
    "cam_obj = cam_obj.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_label, aff_label = obj_label[0, 1:].numpy(), aff_label[0, 1:].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "arrays used as indices must be of integer (or boolean) type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-fe27c478a859>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcam_obj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobj_label\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: arrays used as indices must be of integer (or boolean) type"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.zeros((1, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.randn((8, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.cat([a, b], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[-0.2858, -0.7670, -0.5304,  ...,  0.1446, -0.3225,  1.4257],\n",
       "         [ 0.5527,  1.3963, -0.5061,  ...,  1.5962, -0.0037, -2.5691],\n",
       "         [ 0.4276, -1.6818, -0.3491,  ..., -0.9040,  0.8153,  0.4837],\n",
       "         ...,\n",
       "         [ 2.2317,  0.0387, -2.0376,  ...,  2.0216, -1.4848, -0.2504],\n",
       "         [-0.4412,  0.4135, -0.1002,  ..., -0.8318, -1.2113,  0.1461],\n",
       "         [ 1.5334, -1.1749, -0.3043,  ...,  0.2135, -1.7319, -0.3815]],\n",
       "\n",
       "        [[ 0.2128,  0.4446,  0.1813,  ..., -0.4390, -0.9295, -0.9911],\n",
       "         [ 1.1174, -1.1594,  0.0392,  ..., -1.4464, -0.6901,  0.9382],\n",
       "         [ 0.2556,  0.8098,  1.9742,  ..., -0.8766, -0.6137, -0.7344],\n",
       "         ...,\n",
       "         [ 0.4632,  0.4602, -0.7305,  ..., -0.2574,  0.7601,  0.1164],\n",
       "         [-1.0381, -1.0261,  0.6930,  ..., -1.3054,  0.9169,  0.4914],\n",
       "         [-0.1790,  1.7114, -0.8724,  ..., -1.2285, -0.7379, -0.7345]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.1221, -0.4779,  0.7796,  ...,  0.5883, -1.3277, -1.4704],\n",
       "         [-0.2020,  0.5579, -0.2340,  ..., -0.0120,  2.2184, -0.0851],\n",
       "         [ 0.2453,  0.4799, -0.3649,  ..., -0.5393, -1.9047,  1.2431],\n",
       "         ...,\n",
       "         [-0.1488, -0.5242, -0.2637,  ..., -1.8193,  1.2346, -0.1028],\n",
       "         [-0.9444,  0.4773,  1.6022,  ...,  0.5894,  0.0553, -0.7217],\n",
       "         [-0.0187,  0.4626,  2.2556,  ...,  1.1887,  0.6956,  2.8424]],\n",
       "\n",
       "        [[ 0.3152, -1.0799,  1.7856,  ...,  0.7180,  1.2832, -0.2260],\n",
       "         [ 0.2628,  0.2322, -1.0356,  ...,  1.4081, -0.1791, -0.2234],\n",
       "         [ 0.0502,  1.2180, -1.6988,  ...,  1.8945, -0.7301,  0.7221],\n",
       "         ...,\n",
       "         [-0.6847, -0.1490, -0.1107,  ..., -2.5804,  0.6016, -1.4602],\n",
       "         [ 1.0706,  0.6991,  0.1374,  ..., -2.0424, -1.1958, -0.3660],\n",
       "         [-0.1350,  0.0541, -1.1663,  ...,  1.0769, -0.0363,  0.7861]],\n",
       "\n",
       "        [[ 2.5432, -0.1323,  0.3195,  ..., -0.9856,  0.8458,  0.3654],\n",
       "         [ 1.3944,  1.2854,  0.8899,  ...,  0.3257,  1.0530,  3.0068],\n",
       "         [-0.1442,  0.7766,  0.9979,  ..., -0.3620,  0.5714,  0.8134],\n",
       "         ...,\n",
       "         [ 0.3367, -0.2723, -0.3636,  ..., -0.8561,  1.7919, -0.7195],\n",
       "         [ 0.9621,  0.4864, -0.0169,  ...,  0.6673, -0.2976, -0.7110],\n",
       "         [-0.0651, -0.7516,  0.0858,  ..., -0.3906,  0.2099, -1.4018]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.zeros((20, 20))\n",
    "b = torch.zeros((20, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.stack([a, b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 20, 20])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
